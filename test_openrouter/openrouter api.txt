OAuth PKCE
Users can connect to OpenRouter in one click using Proof Key for Code Exchange (PKCE). Here's an example, and here's a step-by-step:
1. Send your user to https://openrouter.ai/auth?callback_url=YOUR_SITE_URL.
2. You can optionally include a code_challenge (random password up to 256 digits) for extra security.
3. 4. For maximum security, we recommend also setting code_challenge_method to S256, and then setting code_challenge to the base64 encoding of the sha256 hash of code_verifier, which you will submit in Step 2. More info in Auth0's docs.
5. 6. Once logged in, they'll be redirected back to your site with a code in the URL
7. Look for the code query parameter, e.g. ?code=...
8. . Make an API call (can be frontend or backend) to exchange the code for a user-controlled API key. And that's it for PKCE!
9. javascript
10. Copy code
fetch("https://openrouter.ai/api/v1/auth/keys", {
  method: 'POST',
  body: JSON.stringify({
    code: $CODE_FROM_QUERY_PARAM,
    code_verifier: $CODE_VERIFIER // Only needed if you sent a code_challenge in Step 1
  })
11. });
12. A fresh API key will be in the result under "key". Store it securely and make OpenAI-style requests (supports streaming as well):
javascript
Copy code
fetch("https://openrouter.ai/api/v1/chat/completions", {
  method: 'POST',
  headers: {
    'Authorization': 'Bearer ' + $OPENROUTER_API_KEY,
    'HTTP-Referer': $YOUR_SITE_URL, // To identify your app
    'X-Title': $YOUR_SITE_NAME
  },
  body: JSON.stringify({
    model: "openai/gpt-3.5-turbo", // Optional (user controls the default)
    messages: [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "Hello!"}
    ]
  })
});
You can use JavaScript or any server-side framework, like Streamlit . The linked example shows multiple models and file Q&A.
________________


API Keys
Users or developers can cover model costs with normal API keys. This allows you to use curl or the OpenAI SDK directly with OpenRouter. Just create an API key, set the api_base, and set a referrer header.
Note: API keys on OpenRouter are more powerful than keys used directly for model APIs. They allow users to set credit limits for apps, and they can be used in OAuth flows.
Example code in Python:
python
Copy code
import openai


openai.api_base = "https://openrouter.ai/api/v1"
openai.api_key = $OPENROUTER_API_KEY


response = openai.ChatCompletion.create(
model="openai/gpt-3.5-turbo", # Optional (user controls the default)
messages=[...],
headers={ "HTTP-Referer": $YOUR_SITE_URL, # To identify your app
          "X-Title": $YOUR_APP_NAME },
)
reply = response.choices[0].message


To extend this Python code for streaming, see this example from OpenAI.
Example code using curl:
shell
Copy code
curl https://openrouter.ai/api/v1/chat/completions \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $OPENROUTER_API_KEY" \
-H "HTTP-Referer: $YOUR_SITE_URL" \
-H "X-Title: $YOUR_SITE_NAME" \
-d '{
  "model": "openai/gpt-4-32k", # Optional
  "messages": [
    {"role": "system", "content": "You are a helpful assistant."}, 
    {"role": "user", "content": "Hello!"}
  ]
}'


________________


Requests & Responses
More docs coming. In the meantime, see the OpenAI Chat API, which is compatible with OpenRouter, with one exception:
Request Body
javascript
Copy code
{
  // If "model" is omitted, the user/payer's default is used
  // Note the required "openai" prefix:
  "model": "openai/gpt-4-32k",
  // Other params are the same as OpenAI, including "stream"
  ...
}
If the model parameter is omitted, the user or payer's default is used. Otherwise, remember to select a value for model from the supported models or API, and include the organization prefix. Server-Sent Events (SSE) are supported as well, to enable streaming.
If the chosen model doesn't support an request parameter (such as functions or logit_bias in non-OpenAI models, or top_k for OpenAI), then the parameter is ignored. The rest are forwarded to the underlying model API. This allows you to use newly-added parameters immediately in OpenRouter (including those three examples).
Response Body
Responses are largely consistent with OpenAI. This means that choices is always an array, even if the model only returns one completion. Each choice will contain a delta property if a stream was requested and a message property otherwise. This makes it easier to use the same code for all models. Note that finish_reason will vary depending on the model provider.


The model property tells you which model was used inside the underlying API. Example:
javascript
Copy code
{
  "choices": [
    {
      "finish_reason": "stop", // Different models provide different reasons here
      "message": { // will be "delta" if streaming
        role: "assistant",
        content: "Hello there!"
      }
    }
  ],
  "model": "gpt-3.5-turbo-0613" // Could also be "claude-1.3-100k", "chat-bison@001", etc, depending on the "model" that ends up being used
}


________________


Error Handling
For errors, OpenRouter returns a JSON response with the following shape:
typescript
Copy code
type ErrorResponse = {
  error: {
    code: number
    message: string
  }
}
The HTTP Response will have the same status code as error.code, forming a request error if:
* Your original request is invalid
* Your API key/account is out of credits
* You did not set stream: true and the LLM returned an error within 15 seconds.
Otherwise, the returned HTTP response status will be 200 and any error occured while the LLM is producing the output will be emitted in the response body or as an SSE data event.
Example code for printing errors in JavaScript:
javascript
Copy code
const request = await fetch("https://openrouter.ai/...")
console.log(request.status) // Will be an error code unless the model started processing your request
const response = await request.json()
console.error(response.error?.status) // Will be an error code
console.error(response.error?.message)


Error Codes
* 400: Bad Request (invalid or missing params, CORS)
* 401: Invalid credentials
* 402: Out of credits
* 403: API key is disabled or credit limit reached
* 408: Your request time out
* 429: You are being rate limited
* 451: Your chosen model requires moderation and your input was flagged
* 502: Your chosen model is down or we received an invalid response from it
________________


Other Frameworks
* Using Streamlit, a way to build and share Python apps example (Github)
* Using LangChain for Python, a composable LLM framework: example (Github)
* Using LangChain.js: example repo (Github)
* javascript
* Copy code
const chat = new ChatOpenAI(
  {
    modelName: "anthropic/claude-instant-v1",
    temperature: 0.8,
    streaming: true,
    openAIApiKey: $OPENROUTER_API_KEY,
  },
  {
    basePath: $OPENROUTER_BASE_URL + "/api/v1",
    baseOptions: {
      headers: {
        "HTTP-Referer": "https://localhost:3000/",
        "X-Title": "Langchain.js Testing",
      },
    },
  }
* );Using the Vercel AI SDK:
* javascript
* Copy code
const config = new Configuration(
  {
    basePath: $OPENROUTER_BASE_URL + "/api/v1",
    apiKey: $OPENROUTER_API_KEY,
    baseOptions: {
      headers: {
          "HTTP-Referer": $YOUR_SITE_URL,
          "X-Title": $YOUR_SITE_NAME
      }
    }
  }
)


const openrouter = new OpenAIApi(config)
________________


3D Objects (beta)
OpenRouter supports text-to-3D Object generation, currently in beta. See supported media models and try a demo. To generate 3D Objects, send a POST request to https://openrouter.ai/api/v1/objects/generations.
shell
Copy code
curl https://openrouter.ai/api/v1/objects/generations \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $OPENROUTER_API_KEY" \
-H "HTTP-Referer: $YOUR_SITE_URL" \
-H "X-Title: $YOUR_SITE_NAME" \
-d '{
  "prompt": "a chair shaped like an avacado", # Required
  "num_inference_steps": 32, # Optional
  "num_outputs": 1, # Optional
  "extension": "ply", # Optional
  "model": "openai/shap-e", # Optional
}'


You should recieve a response of type MediaResponse:
typescript
Copy code
//Each generation will contain either a base64 string or a hosted url, or both.
interface MediaOutput {
  uri?: string; //base64 string
  url?: string; //hosted url
}


interface MediaResponse {
  generations: MediaOutput[];
}